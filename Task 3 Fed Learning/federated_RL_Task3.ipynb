{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEDRL: Running 3 agents for HalfCheetah env for 150 training iterations in parallel following PPO algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/miniconda3/envs/rllib/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air, tune\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "\n",
    "import gym\n",
    "\n",
    "MultiHalfCheetah = make_multi_agent(\"HalfCheetah-v3\")\n",
    "\n",
    "\n",
    "# different ctrl_cost_weight and reset_noise_scale for 3 agents\n",
    "paramlist = [\n",
    "    [0.1, 0.1],\n",
    "    [0.5, 0.3],\n",
    "    [0.6, 0.8]\n",
    "]\n",
    "\n",
    "class WrappedMultiHalfCheetah(MultiHalfCheetah):\n",
    "    def __init__(self, config: EnvContext = None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        num = config.pop(\"num_agents\", 1)\n",
    "\n",
    "        if isinstance(\"HalfCheetah-v3\", str):\n",
    "            self.agents = [gym.make(\"HalfCheetah-v3\", ctrl_cost_weight=paramlist[i][0], reset_noise_scale=paramlist[i][1]) for i in range(num)]\n",
    "        self.dones = set()\n",
    "        self.observation_space = self.agents[0].observation_space\n",
    "        self.action_space = self.agents[0].action_space\n",
    "        self._agent_ids = set(range(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    }
   ],
   "source": [
    "env = MultiHalfCheetah()\n",
    "\n",
    "# Running 3 agents for HalfCheetah env for 150 training iterations in parallel\n",
    "config = PPOConfig()\\\n",
    "    .environment(WrappedMultiHalfCheetah, env_config={\"num_agents\": 3})\\\n",
    "    .rollouts(num_rollout_workers=1)\\\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_0\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
    "            ),\n",
    "            \"policy_1\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
    "            ),\n",
    "            \"policy_2\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.92}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-10 22:45:18</td></tr>\n",
       "<tr><td>Running for: </td><td>00:51:37.04        </td></tr>\n",
       "<tr><td>Memory:      </td><td>24.5/39.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/19.97 GiB heap, 0.0/9.99 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                             </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_WrappedMultiHalfCheetah_071f9_00000</td><td>TERMINATED</td><td>172.25.0.55:132364</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">            3075</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">-679.109</td><td style=\"text-align: right;\">             886.149</td><td style=\"text-align: right;\">            -4940.43</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:53:41,951\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-05-10 21:53:41,952\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-05-10 21:53:41,952\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(pid=132364)\u001b[0m /home/rakesh/miniconda3/envs/rllib/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=132364)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PPO pid=132364)\u001b[0m 2023-05-10 21:53:46,438\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=132364)\u001b[0m 2023-05-10 21:53:46,438\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=132364)\u001b[0m 2023-05-10 21:53:46,716\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=132441)\u001b[0m /home/rakesh/miniconda3/envs/rllib/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=132441)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=132441)\u001b[0m <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=132441)\u001b[0m 2023-05-10 21:53:52,199\tWARNING env.py:247 -- Your MultiAgentEnv <WrappedMultiHalfCheetah instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=132364)\u001b[0m 2023-05-10 21:53:56,695\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=132441)\u001b[0m 2023-05-10 21:53:56,756\tWARNING deprecation.py:47 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=132364)\u001b[0m 2023-05-10 21:54:09,070\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2023-05-10 22:45:19,729\tINFO tune.py:762 -- Total run time: 3097.80 seconds (3096.99 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = {\n",
    "    \"training_iteration\": 150, \n",
    "}\n",
    "tuner = tune.Tuner(\n",
    "        \"PPO\",\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"episode_reward_mean\",\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=air.RunConfig(local_dir='./results_fedRL' ,name=\"FEDRL_HALFCHEETAH_PPO\" ,stop=stopping_criteria, verbose=1),\n",
    "    )\n",
    "results = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running 3 agents for Ant env for 100 training iterations in parallel folling PPO algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air, tune\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "\n",
    "import os, random, gym\n",
    "\n",
    "MultiAnt = make_multi_agent(\"Ant-v3\")\n",
    "\n",
    "\n",
    "# different ctrl_cost_weight, reset_noise_scale and contact_cost_weight for 3 agents\n",
    "paramlist = [\n",
    "    [0.1, 0.1, 0.2],\n",
    "    [0.5, 0.3, 0.4],\n",
    "    [0.6, 0.8, 0.5]\n",
    "]\n",
    "\n",
    "class WrappedMultiAnt(MultiAnt):\n",
    "    def __init__(self, config: EnvContext = None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        num = config.pop(\"num_agents\", 1)\n",
    "\n",
    "        if isinstance(\"Ant-v3\", str):\n",
    "            self.agents = [gym.make(\"Ant-v3\", ctrl_cost_weight=paramlist[i][0], reset_noise_scale=paramlist[i][1], contact_cost_weight=paramlist[i][2]) for i in range(num)]\n",
    "        self.dones = set()\n",
    "        self.observation_space = self.agents[0].observation_space\n",
    "        self.action_space = self.agents[0].action_space\n",
    "        self._agent_ids = set(range(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiAnt()\n",
    "\n",
    "# Running 3 agents for Ant env for 100 training iterations in parallel\n",
    "config = PPOConfig()\\\n",
    "    .environment(WrappedMultiAnt, env_config={\"num_agents\": 3})\\\n",
    "    .rollouts(num_rollout_workers=1)\\\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_0\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
    "            ),\n",
    "            \"policy_1\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
    "            ),\n",
    "            \"policy_2\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.92}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-10 23:04:36</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:17.01        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.8/39.1 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/19.97 GiB heap, 0.0/9.99 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_WrappedMultiAnt_3da7a_00000</td><td>TERMINATED</td><td>172.25.0.55:135479</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1134.46</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 957.444</td><td style=\"text-align: right;\">             1490.41</td><td style=\"text-align: right;\">             65.4109</td><td style=\"text-align: right;\">            886.71</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 22:45:19,907\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-05-10 22:45:19,909\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-05-10 22:45:19,909\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(pid=135479)\u001b[0m /home/rakesh/miniconda3/envs/rllib/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=135479)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(PPO pid=135479)\u001b[0m 2023-05-10 22:45:25,044\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=135479)\u001b[0m 2023-05-10 22:45:25,045\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=135479)\u001b[0m 2023-05-10 22:45:25,391\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=135556)\u001b[0m /home/rakesh/miniconda3/envs/rllib/lib/python3.10/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=135556)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=135556)\u001b[0m <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=135556)\u001b[0m 2023-05-10 22:45:31,583\tWARNING env.py:247 -- Your MultiAgentEnv <WrappedMultiAnt instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(PPO pid=135479)\u001b[0m 2023-05-10 22:45:36,975\tINFO trainable.py:172 -- Trainable.setup took 11.586 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=135479)\u001b[0m 2023-05-10 22:45:36,975\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=135556)\u001b[0m 2023-05-10 22:45:37,054\tWARNING deprecation.py:47 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=135479)\u001b[0m 2023-05-10 22:45:48,342\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2023-05-10 23:04:37,410\tINFO tune.py:762 -- Total run time: 1157.52 seconds (1156.96 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = {\n",
    "    \"training_iteration\": 100, \n",
    "}\n",
    "tuner = tune.Tuner(\n",
    "        \"PPO\",\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"episode_reward_mean\",\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=air.RunConfig(local_dir='./results_fedRL' ,name=\"FEDRL_ANT_PPO\" ,stop=stopping_criteria, verbose=1),\n",
    "    )\n",
    "results = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to transform privacy preserving Federated setting where the meta policy is learned at the global level without sharing the samples generated by each individual agent.\n",
    "MBMPO closely works like federated leaning. It uses an ensembles of transition dynamics models for learning. Its task is to learn a policy that can perform well across the ensemble. So we could have several parallel agents in a federated setting and collectively build a global model without sharing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
